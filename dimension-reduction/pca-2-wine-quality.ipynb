{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on wine quality data\n",
    "We like this data, because it is all numeric data.\n",
    "It looks like this\n",
    "\n",
    "```\n",
    "\"fixed acidity\";\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n",
    "7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5\n",
    "7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5\n",
    "7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;9.8;5\n",
    "```\n",
    "\n",
    "Check data in *datasets/wine-quality*\n",
    "- winequality-red.csv\n",
    "- winequality-white.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "## red wine\n",
    "data_location = '../data/wine-quality/winequality-red.csv'\n",
    "data_url = 'https://elephantscale-public.s3.amazonaws.com/data/wine-quality/winequality-red.csv'\n",
    "\n",
    "## white wine\n",
    "# data_location= '../data/wine-quality/winequality-white.csv'\n",
    "# data_location =  'https://elephantscale-public.s3.amazonaws.com/data/wine-quality/winequality-white.csv'\n",
    "\n",
    "if not os.path.exists (data_location):\n",
    "    data_location = os.path.basename(data_location)\n",
    "    if not os.path.exists(data_location):\n",
    "        print(\"Downloading : \", data_url)\n",
    "        urllib.request.urlretrieve(data_url, data_location)\n",
    "print('data_location:', data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(data_location, sep=\";\")\n",
    "dataset = dataset.dropna()\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Basic data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_remove = 'quality'\n",
    "\n",
    "dataset2 = dataset.drop(column_to_remove, axis=1)\n",
    "\n",
    "print(\"original data columns  \", len(dataset.columns))\n",
    "\n",
    "features = list(dataset2)\n",
    "print(\"features: \" + str(features))\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic data analytics\n",
    "dataset2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Create feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = dataset2\n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Correlation Matrix of original data\n",
    "Do see any correlation?\n",
    "\n",
    "Which columns have the strongest correlation (positive or negative) with\n",
    "quality?\n",
    "\n",
    "Write some python code that will find and rank all columns by correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.corr()\n",
    "\n",
    "## TODO: Rank columns by correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Scale Data\n",
    "We need to scale data before PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = (feature_vector - feature_vector.mean()) / feature_vector.std()\n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of principal components \n",
    "\n",
    "## TODO with 5 PC \n",
    "num_pc = ???\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = num_pc)\n",
    "pca.fit(feature_vector)\n",
    "transformed_v = pca.transform(feature_vector)\n",
    "transformed_v_df = pd.DataFrame(transformed_v, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "transformed_v_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Correlation Matrix for Principal Components\n",
    "These should be very small (close to zero!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_v_df.corr().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Calculate PC Variance\n",
    "\n",
    "We started with 5 PCs.  \n",
    "How much coverage (variance) are we getting?\n",
    "\n",
    "Play with **num_pc** in Step-6 to get 90% coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## variance\n",
    "variance = pca.explained_variance_ratio_\n",
    "print(variance)\n",
    "print (\"Original data had {} features,  principal components {}\".format(len(dataset2.columns), num_pc))\n",
    "print(\"Cumulative Explained Variance: \" + str(np.cumsum(variance)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Screeplot\n",
    "Screeplot goes from 0.0  to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "sing_vals = np.arange(num_pc) + 1\n",
    "plt.plot(np.arange(num_pc) + 1, np.cumsum(variance), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "\n",
    "\n",
    "leg = plt.legend(['Explained Variance'], loc='best', borderpad=0.3, \n",
    "                 shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n",
    "                 markerscale=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in range(0,5):\n",
    "    # Let's get the top components of PC1:\n",
    "    print(\"top components of PC\" + str(x+1) + \":\")\n",
    "    rel_values = np.abs(pca.components_[x])/np.sum(np.abs(pca.components_[x]))\n",
    "    print(\"Feature Names: \" + str([features[i] for i in np.argsort(-rel_values)[:3]]))\n",
    "    print(\"Percentages: \" + str(rel_values[np.argsort(-rel_values)[:3]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Biplot\n",
    "\n",
    "Let's reduce dimensions down to 2 dimensions, and then we can do our biplot.  A biplot plots 2 PCA'ed dimensions, and then also projects the original feature vector onto those two axes.  This helps us see and visualize how the principal components are related to the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def biplot(score,coeff,y,labels=None):\n",
    "    plt.rcParams['figure.figsize'] = [15, 10]\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "    plt.xlim(-0.6,0.8)\n",
    "    plt.ylim(-0.6,0.8)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_2d = PCA(n_components = 2)\n",
    "\n",
    "x_new = pca_2d.fit_transform(feature_vector)\n",
    "\n",
    "\n",
    "# Let's do a biplot of a PCA = 2 dimensions\n",
    "biplot(x_new[:,0:2],np.transpose(pca.components_[0:2, :]),dataset['quality'],labels=features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret biplot\n",
    "\n",
    "What does the biplot tell us?  Of the two principal components, which of the original features is strongly captured in PC1?  What about PC2?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
